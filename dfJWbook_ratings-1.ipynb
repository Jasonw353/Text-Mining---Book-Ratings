{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9f6067a-ee35-49ba-a73c-5ed125cbef71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Key Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0d55e7a-4a86-43bb-938d-b46d62823781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read book_ratings.csv\n",
    "book_ratings = pd.read_csv(\"book_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1fa64b0f-50a3-476b-92cd-27c3d0bf8880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f717d025-6788-49dc-8bfe-ab2a3befda99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].astype(str).str.lower()\n",
    "book_ratings['review/text'] = book_ratings['review/text'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16b95bb5-8948-4cde-ad97-62b3d9311f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].apply(word_tokenize)\n",
    "book_ratings['review/text'] = book_ratings['review/text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cac4da5b-be8e-4278-aea3-b0568ec39f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].apply(lambda x: [word for word in x if word.isalnum()])\n",
    "book_ratings['review/text'] = book_ratings['review/text'].apply(lambda x: [word for word in x if word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d369397e-3dbf-4956-b730-36a039354604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "book_ratings['review/text'] = book_ratings['review/text'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0d8ca9c-039d-4639-b3d7-79b61f6774ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "book_ratings['review/text'] = book_ratings['review/text'].apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0050379a-1d93-4f67-b09c-9d6e9d945d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing Special Characters and Numbers\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].apply(lambda x: [re.sub(r'[^a-zA-Z]', '', str(word)) for word in x])\n",
    "book_ratings['review/text'] = book_ratings['review/text'].apply(lambda x: [re.sub(r'[^a-zA-Z]', '', str(word)) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "acbb263c-da06-4001-86df-cc38193e26ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_stop_words = {\"is\", \"in\", \"would\", \"may\", \"must\", \"one\", \"upon\", \"might\", \"shall\", \"could\"}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = stop_words.union(custom_stop_words)\n",
    "\n",
    "book_ratings['review/summary'] = book_ratings['review/summary'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "book_ratings['review/text'] = book_ratings['review/text'].apply(lambda x: [word for word in x if word not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7caa742a-a39d-4132-b9d5-931d063839e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Read the CSV file\n",
    "book_ratings = pd.read_csv(\"book_ratings.csv\")\n",
    "\n",
    "# Create corpus for \"review/summary\" and \"review/text\"\n",
    "corpus_text = [doc.split() for doc in book_ratings[\"review/text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8bcda92c-3896-4a49-8ee4-4511b75fedb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dictionary for \"review/text\"\n",
    "text_dictionary = corpora.Dictionary(corpus_text)\n",
    "\n",
    "# Filter out terms that appear in fewer than 2 documents or more than 75% of the documents\n",
    "text_dictionary.filter_extremes(no_below=2, no_above=0.75)\n",
    "\n",
    "# Convert \"review/text\" corpus into document-term matrix\n",
    "text_corpus = [text_dictionary.doc2bow(tokens) for tokens in corpus_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1fee37ab-6574-4045-bd92-7a5a08644c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Train TF-IDF model\n",
    "tfidf = models.TfidfModel(text_corpus)\n",
    "text_corpus_tfidf = tfidf[text_corpus]\n",
    "\n",
    "# Apply SVD to extract 5 components\n",
    "n_SVD = 5\n",
    "SVD_model = models.LsiModel(text_corpus_tfidf, id2word=text_dictionary, num_topics=n_SVD)\n",
    "SVD = SVD_model[text_corpus_tfidf]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the book_ratings.csv file\n",
    "book_ratings = pd.read_csv(\"book_ratings.csv\")\n",
    "\n",
    "# Extract the non-text columns\n",
    "non_text_columns = book_ratings[['User_id', 'review/helpfulness']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "91543b06-c077-4561-adc9-08e79c55ce95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.044458</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>-0.001194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.227860</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>-0.086918</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>-0.010232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.172844</td>\n",
       "      <td>-0.017331</td>\n",
       "      <td>-0.020711</td>\n",
       "      <td>0.064680</td>\n",
       "      <td>-0.010724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.135117</td>\n",
       "      <td>0.054652</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>-0.017435</td>\n",
       "      <td>-0.054039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.219155</td>\n",
       "      <td>-0.005209</td>\n",
       "      <td>-0.078767</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.007763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.163187</td>\n",
       "      <td>-0.086378</td>\n",
       "      <td>-0.092925</td>\n",
       "      <td>0.057052</td>\n",
       "      <td>-0.025490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.097399</td>\n",
       "      <td>-0.125608</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>-0.013534</td>\n",
       "      <td>-0.027389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.143504</td>\n",
       "      <td>0.028947</td>\n",
       "      <td>0.056614</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>-0.074167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.084141</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>-0.006905</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>-0.040530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.142592</td>\n",
       "      <td>0.056084</td>\n",
       "      <td>0.071676</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>-0.097222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4\n",
       "0   -0.044458  0.017166  0.006512  0.022264 -0.001194\n",
       "1   -0.227860  0.026546 -0.086918  0.008854 -0.010232\n",
       "2   -0.172844 -0.017331 -0.020711  0.064680 -0.010724\n",
       "3   -0.135117  0.054652 -0.000707 -0.017435 -0.054039\n",
       "4   -0.219155 -0.005209 -0.078767  0.008632  0.007763\n",
       "..        ...       ...       ...       ...       ...\n",
       "995 -0.163187 -0.086378 -0.092925  0.057052 -0.025490\n",
       "996 -0.097399 -0.125608  0.007111 -0.013534 -0.027389\n",
       "997 -0.143504  0.028947  0.056614  0.041186 -0.074167\n",
       "998 -0.084141  0.004337 -0.006905  0.017827 -0.040530\n",
       "999 -0.142592  0.056084  0.071676 -0.001183 -0.097222\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert results into array\n",
    "svd_array = gensim.matutils.corpus2csc(SVD).T.toarray()\n",
    "\n",
    "# convert results to data frame\n",
    "svd_df = pd.DataFrame(svd_array)\n",
    "\n",
    "# show SVD results - reduced vector representation of the documents\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b42e4050-5cf8-490c-acc9-257653ad7e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6\n",
      "0    18.96  0.142857 -0.044458  0.017166  0.006512  0.022264 -0.001194\n",
      "1    14.04  0.950000 -0.227860  0.026546 -0.086918  0.008854 -0.010232\n",
      "2    19.57       NaN -0.172844 -0.017331 -0.020711  0.064680 -0.010724\n",
      "3    43.94  1.000000 -0.135117  0.054652 -0.000707 -0.017435 -0.054039\n",
      "4    15.27  0.666667 -0.219155 -0.005209 -0.078767  0.008632  0.007763\n",
      "..     ...       ...       ...       ...       ...       ...       ...\n",
      "995  17.89  1.000000 -0.163187 -0.086378 -0.092925  0.057052 -0.025490\n",
      "996  14.95  1.000000 -0.097399 -0.125608  0.007111 -0.013534 -0.027389\n",
      "997  12.95       NaN -0.143504  0.028947  0.056614  0.041186 -0.074167\n",
      "998  10.79       NaN -0.084141  0.004337 -0.006905  0.017827 -0.040530\n",
      "999  10.95  1.000000 -0.142592  0.056084  0.071676 -0.001183 -0.097222\n",
      "\n",
      "[1000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the book_ratings.csv file\n",
    "book_ratings = pd.read_csv(\"book_ratings.csv\")\n",
    "\n",
    "# Select the non-text columns\n",
    "non_text_columns = book_ratings[['Price', 'review/helpfulness']]\n",
    "\n",
    "# Convert non-text columns to arrays\n",
    "non_text_array = non_text_columns.values\n",
    "\n",
    "# Combine non-text columns with SVD components\n",
    "combined_array = np.concatenate((non_text_array, svd_array), axis=1)\n",
    "\n",
    "# Convert the combined array to a DataFrame\n",
    "combined_df = pd.DataFrame(combined_array)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18634c10-fc89-4b3c-9370-3d58fdc2867f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.1417805334337736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Extract the \"review/score\" column\n",
    "y = book_ratings[\"review/score\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "Model_1 = DecisionTreeRegressor()\n",
    "Model_1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = Model_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "263aef84-1485-4ce1-8dc7-8eb5fdd071d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  0  17]\n",
      " [  0 183]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Discretize the \"review/score\" into categories\n",
    "y_train_class = pd.cut(y_train, bins=[-np.inf, 2.5, 5.0, np.inf], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Train the decision tree model for classification\n",
    "Model_1_class = DecisionTreeClassifier()\n",
    "Model_1_class.fit(X_train, y_train_class)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class = Model_1_class.predict(X_test)\n",
    "y_test_class = pd.cut(y_test, bins=[-np.inf, 2.5, 5.0, np.inf], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Evaluate the model using confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7e53cfe4-fd98-447e-91ce-5878fb9fc635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Train TF-IDF model\n",
    "tfidf = models.TfidfModel(text_corpus)\n",
    "text_corpus_tfidf = tfidf[text_corpus]\n",
    "\n",
    "# Apply SVD to extract 8 components\n",
    "n_SVD = 8\n",
    "SVD_model = models.LsiModel(text_corpus_tfidf, id2word=text_dictionary, num_topics=n_SVD)\n",
    "SVD = SVD_model[text_corpus_tfidf]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the book_ratings.csv file\n",
    "book_ratings = pd.read_csv(\"book_ratings.csv\")\n",
    "\n",
    "# Extract the non-text columns\n",
    "non_text_columns = book_ratings[['User_id', 'review/helpfulness']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "28d2b733-bae9-4c7a-b0b6-736ee99756f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.044431</td>\n",
       "      <td>-0.021650</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>-0.020095</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>0.034956</td>\n",
       "      <td>0.009291</td>\n",
       "      <td>-0.023217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.227810</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>-0.086751</td>\n",
       "      <td>-0.015460</td>\n",
       "      <td>-0.025954</td>\n",
       "      <td>0.017204</td>\n",
       "      <td>-0.019817</td>\n",
       "      <td>0.020377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.172796</td>\n",
       "      <td>0.017253</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>-0.063272</td>\n",
       "      <td>-0.016479</td>\n",
       "      <td>-0.043693</td>\n",
       "      <td>0.054208</td>\n",
       "      <td>-0.030815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.135180</td>\n",
       "      <td>-0.061913</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>-0.051935</td>\n",
       "      <td>0.059716</td>\n",
       "      <td>-0.036561</td>\n",
       "      <td>0.012752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.219112</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>-0.071628</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>-0.029799</td>\n",
       "      <td>0.010689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.163194</td>\n",
       "      <td>0.083621</td>\n",
       "      <td>-0.095028</td>\n",
       "      <td>-0.059430</td>\n",
       "      <td>-0.023682</td>\n",
       "      <td>-0.030185</td>\n",
       "      <td>0.053305</td>\n",
       "      <td>-0.003794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.097285</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.007289</td>\n",
       "      <td>-0.027124</td>\n",
       "      <td>0.067648</td>\n",
       "      <td>0.040329</td>\n",
       "      <td>-0.047133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.143568</td>\n",
       "      <td>-0.024855</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>-0.047158</td>\n",
       "      <td>-0.088821</td>\n",
       "      <td>-0.051233</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>-0.006587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.084143</td>\n",
       "      <td>-0.005409</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>-0.022515</td>\n",
       "      <td>-0.053508</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>-0.003666</td>\n",
       "      <td>-0.005082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.142545</td>\n",
       "      <td>-0.060386</td>\n",
       "      <td>0.068624</td>\n",
       "      <td>-0.009436</td>\n",
       "      <td>-0.102657</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>-0.088439</td>\n",
       "      <td>0.013215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.044431 -0.021650  0.002172 -0.020095 -0.002945  0.034956  0.009291   \n",
       "1   -0.227810 -0.024358 -0.086751 -0.015460 -0.025954  0.017204 -0.019817   \n",
       "2   -0.172796  0.017253 -0.017151 -0.063272 -0.016479 -0.043693  0.054208   \n",
       "3   -0.135180 -0.061913  0.000099  0.013374 -0.051935  0.059716 -0.036561   \n",
       "4   -0.219112  0.002204 -0.071628  0.004215  0.006191  0.022200 -0.029799   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995 -0.163194  0.083621 -0.095028 -0.059430 -0.023682 -0.030185  0.053305   \n",
       "996 -0.097285  0.125400  0.003377  0.007289 -0.027124  0.067648  0.040329   \n",
       "997 -0.143568 -0.024855  0.056297 -0.047158 -0.088821 -0.051233  0.011546   \n",
       "998 -0.084143 -0.005409  0.004295 -0.022515 -0.053508  0.002776 -0.003666   \n",
       "999 -0.142545 -0.060386  0.068624 -0.009436 -0.102657  0.010418 -0.088439   \n",
       "\n",
       "            7  \n",
       "0   -0.023217  \n",
       "1    0.020377  \n",
       "2   -0.030815  \n",
       "3    0.012752  \n",
       "4    0.010689  \n",
       "..        ...  \n",
       "995 -0.003794  \n",
       "996 -0.047133  \n",
       "997 -0.006587  \n",
       "998 -0.005082  \n",
       "999  0.013215  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert results into array\n",
    "svd_array = gensim.matutils.corpus2csc(SVD).T.toarray()\n",
    "\n",
    "# convert results to data frame\n",
    "svd_df = pd.DataFrame(svd_array)\n",
    "\n",
    "# show SVD results - reduced vector representation of the documents\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c8f02033-dfe4-4f6e-9beb-64b3b6a31be5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6  \\\n",
      "0    18.96  0.142857 -0.044431 -0.021650  0.002172 -0.020095 -0.002945   \n",
      "1    14.04  0.950000 -0.227810 -0.024358 -0.086751 -0.015460 -0.025954   \n",
      "2    19.57       NaN -0.172796  0.017253 -0.017151 -0.063272 -0.016479   \n",
      "3    43.94  1.000000 -0.135180 -0.061913  0.000099  0.013374 -0.051935   \n",
      "4    15.27  0.666667 -0.219112  0.002204 -0.071628  0.004215  0.006191   \n",
      "..     ...       ...       ...       ...       ...       ...       ...   \n",
      "995  17.89  1.000000 -0.163194  0.083621 -0.095028 -0.059430 -0.023682   \n",
      "996  14.95  1.000000 -0.097285  0.125400  0.003377  0.007289 -0.027124   \n",
      "997  12.95       NaN -0.143568 -0.024855  0.056297 -0.047158 -0.088821   \n",
      "998  10.79       NaN -0.084143 -0.005409  0.004295 -0.022515 -0.053508   \n",
      "999  10.95  1.000000 -0.142545 -0.060386  0.068624 -0.009436 -0.102657   \n",
      "\n",
      "            7         8         9  \n",
      "0    0.034956  0.009291 -0.023217  \n",
      "1    0.017204 -0.019817  0.020377  \n",
      "2   -0.043693  0.054208 -0.030815  \n",
      "3    0.059716 -0.036561  0.012752  \n",
      "4    0.022200 -0.029799  0.010689  \n",
      "..        ...       ...       ...  \n",
      "995 -0.030185  0.053305 -0.003794  \n",
      "996  0.067648  0.040329 -0.047133  \n",
      "997 -0.051233  0.011546 -0.006587  \n",
      "998  0.002776 -0.003666 -0.005082  \n",
      "999  0.010418 -0.088439  0.013215  \n",
      "\n",
      "[1000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the book_ratings.csv file\n",
    "book_ratings = pd.read_csv(\"book_ratings.csv\")\n",
    "\n",
    "# Select the non-text columns\n",
    "non_text_columns = book_ratings[['Price', 'review/helpfulness']]\n",
    "\n",
    "# Convert non-text columns to arrays\n",
    "non_text_array = non_text_columns.values\n",
    "\n",
    "# Combine non-text columns with SVD components\n",
    "combined_array = np.concatenate((non_text_array, svd_array), axis=1)\n",
    "\n",
    "# Convert the combined array to a DataFrame\n",
    "combined_df = pd.DataFrame(combined_array)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "227209d0-8a7a-424e-84e8-78a60a5db616",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.0134984876999307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Extract the \"review/score\" column\n",
    "y = book_ratings[\"review/score\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "Model_2 = DecisionTreeRegressor()\n",
    "Model_2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = Model_1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "80a421c4-980f-4276-93e3-a32342ecb676",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  1  16]\n",
      " [ 10 173]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Discretize the \"review/score\" into categories\n",
    "y_train_class = pd.cut(y_train, bins=[-np.inf, 2.5, 5.0, np.inf], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Train the decision tree model for classification\n",
    "Model_1_class = DecisionTreeClassifier()\n",
    "Model_1_class.fit(X_train, y_train_class)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class = Model_1_class.predict(X_test)\n",
    "y_test_class = pd.cut(y_test, bins=[-np.inf, 2.5, 5.0, np.inf], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Evaluate the model using confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734be87-e7c7-4dce-9c63-24c9d0afa894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
